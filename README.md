# VaryOn AI Chat Platform

Open WebUI + Ollama setup for chat.varyon.ai

## Setup Instructions

1. Clone this repository
2. Run `docker-compose up -d`
3. Configure Nginx reverse proxy
4. Download models with `docker exec ollama ollama pull llama3.2:3b`

## Local Development

- Copy configuration files to your local environment
- Modify URLs and ports as needed
- Test locally before deploying
Last updated: Thu Aug 28 02:02:40 EST 2025
